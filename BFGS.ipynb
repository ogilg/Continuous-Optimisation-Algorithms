{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFGS (Broyden, Fletcher, Goldfarb and Shanno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quasi-Newton method which approximates the Hessian at each step. We reset the Hessian to I if curvature condition fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a numpy array\n",
    "def f(x):\n",
    "    return np.array([np.exp(x[0])+np.exp((-1)*x[0]+4.) + x[1]**2])\n",
    "    #return (x-7)*(x-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y,x):\n",
    "    return np.linalg.norm(y-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general class of linesearch iterative methods for continuous optimisation\n",
    "class LineSearch:\n",
    "    \n",
    "    def __init__(self,backtracking,step):\n",
    "        self.backtracking = backtracking\n",
    "        self.step = step\n",
    "        \n",
    "        self.sigma = 0.1\n",
    "        self.tol = 0.00000001\n",
    "        self.rho = 1/2\n",
    "        \n",
    "    \n",
    "    def Armijo_test(self,f,x,step,direction):\n",
    "        a = f(x+step*direction)\n",
    "        b = f(x) - self.sigma*step*np.dot(direction,direction)\n",
    "        return (a < b).all()\n",
    "    \n",
    "    #starts from previous step and finds suitable step length by testing for the Armijo condition\n",
    "    def find_step(self,f,x,step,direction):\n",
    "        iters =0\n",
    "        \n",
    "        while not self.Armijo_test(f,x,step,direction):\n",
    "            step = self.rho * step\n",
    "            iters +=1\n",
    "            if iters > 10000:\n",
    "                return None\n",
    "        return step\n",
    "        \n",
    "    #minimise f from starting point x\n",
    "    def find_minimum(self,f,x):\n",
    "        #executes steps until convergence\n",
    "        prev_x = 10*10*10\n",
    "        iters = 0\n",
    "        step = self.step\n",
    "        \n",
    "        grad = elementwise_grad(f)(x)\n",
    "        #set initial direction to steepest descent\n",
    "        direction = np.negative(grad)\n",
    "        #set initial hessian to I so that first step is gradient descent\n",
    "        hessian = np.identity(x.size)\n",
    "        \n",
    "        \n",
    "        while np.linalg.norm(x-prev_x) > self.tol:\n",
    "            \n",
    "            prev_grad = grad\n",
    "            grad = elementwise_grad(f)(x)\n",
    "            delta_grad = grad - prev_grad\n",
    "            delta_x = x - prev_x\n",
    "            \n",
    "            hessian = BFGS(f,delta_x,delta_grad, hessian)\n",
    "            direction = np.dot(np.linalg.inv(hessian),np.negative(grad))\n",
    "            \n",
    "            if self.backtracking :\n",
    "                step = self.find_step(f,x,step,direction)\n",
    "            if step == None:\n",
    "                print(\"step not found\")\n",
    "                break\n",
    "\n",
    "            prev_x = x\n",
    "            \n",
    "            #we use previous direction to compute the next            \n",
    "            x = x + step*direction\n",
    "            \n",
    "            if iters > 10000:\n",
    "                print(\"iteration limit reached\")\n",
    "                return\n",
    "            else:\n",
    "                #if iters % 10 == 0:\n",
    "                print(\"progress\")\n",
    "                print(x)\n",
    "                print(f(x))\n",
    "                print()\n",
    "                iters +=1\n",
    "        print(\"converged in %d iterations to\" %iters)\n",
    "        print(\"x value :\", x)\n",
    "        print(\"out : \", f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ||| x float |||\n",
    "def BFGS(f,delta_x,delta_g, prev_hessian):\n",
    "    if np.dot(delta_x.T,delta_g) > 0:\n",
    "        q1 = np.dot(delta_g,delta_g.T)/np.dot(delta_g.T,delta_g)\n",
    "\n",
    "        dxHessian = np.dot(delta_x.T, prev_hessian)\n",
    "\n",
    "        q2 = np.dot(np.dot(prev_hessian,delta_x),dxHessian)/np.dot(dxHessian,delta_x)\n",
    "\n",
    "        new_hessian = prev_hessian + q1 - q2\n",
    "        return new_hessian\n",
    "    else:\n",
    "        return np.identity(delta_x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfgs = LineSearch(step = 100.,backtracking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress\n",
      "[ 1.         10.52121907 15.75788956 20.99456004 26.23123053 31.46790102\n",
      " 36.70457151 41.94124199 47.17791248 52.41458297]\n",
      "110.05713508471928\n",
      "\n",
      "progress\n",
      "[  1.          20.04243814  30.51577911  40.98912009  51.46246106\n",
      "  61.93580204  72.40914301  82.88248399  93.35582496 103.82916594]\n",
      "10.057135084719286\n",
      "\n",
      "progress\n",
      "[  1.          21.23259052  32.36051531  43.48844009  54.61636488\n",
      "  65.74428966  76.87221445  88.00013924  99.12806402 110.25598881]\n",
      "2.4428649152807083\n",
      "\n",
      "progress\n",
      "[  1.          20.93505242  31.89933126  42.86361009  53.82788892\n",
      "  64.79216776  75.75644659  86.72072542  97.68500426 108.64928309]\n",
      "0.6821350847192944\n",
      "\n",
      "progress\n",
      "[  1.          21.00943695  32.01462727  43.01981759  54.02500791\n",
      "  65.03019823  76.03538856  87.04057888  98.0457692  109.05095952]\n",
      "0.0991149152807002\n",
      "\n",
      "progress\n",
      "[  1.          21.00013888  32.00021527  43.00029165  54.00036804\n",
      "  65.00044442  76.00052081  87.0005972   98.00067358 109.00074997]\n",
      "0.0014586652806928555\n",
      "\n",
      "progress\n",
      "[  1.          20.9999936   31.99999008  42.99998656  53.99998304\n",
      "  64.99997952  75.999976    86.99997248  97.99996896 108.99996544]\n",
      "6.721362555487234e-05\n",
      "\n",
      "progress\n",
      "[  1.          21.00000268  32.00000415  43.00000563  54.0000071\n",
      "  65.00000858  76.00001005  87.00001153  98.000013   109.00001448]\n",
      "2.815380608612438e-05\n",
      "\n",
      "progress\n",
      "[  1.          20.99999814  31.99999712  42.9999961   53.99999507\n",
      "  64.99999405  75.99999303  86.999992    97.99999098 108.99998996]\n",
      "1.9529909735608635e-05\n",
      "\n",
      "progress\n",
      "[  1.          21.00000041  32.00000064  43.00000086  54.00000109\n",
      "  65.00000131  76.00000154  87.00000177  98.00000199 109.00000222]\n",
      "4.3119481774058346e-06\n",
      "\n",
      "progress\n",
      "[  1.          20.99999984  31.99999976  42.99999967  53.99999958\n",
      "  64.9999995   75.99999941  86.99999933  97.99999924 108.99999915]\n",
      "1.648516302488352e-06\n",
      "\n",
      "progress\n",
      "[  1.          21.00000013  32.0000002   43.00000027  54.00000034\n",
      "  65.00000041  76.00000048  87.00000055  98.00000061 109.00000068]\n",
      "1.3317159352008446e-06\n",
      "\n",
      "progress\n",
      "[  1.          20.99999998  31.99999998  42.99999997  53.99999996\n",
      "  64.99999995  75.99999994  86.99999994  97.99999993 108.99999992]\n",
      "1.584001831617311e-07\n",
      "\n",
      "progress\n",
      "[  1.          21.          32.          43.00000001  54.00000001\n",
      "  65.00000001  76.00000001  87.00000001  98.00000001 109.00000001]\n",
      "2.786433012198553e-08\n",
      "\n",
      "progress\n",
      "[  1.          21.          32.          43.          54.\n",
      "  64.99999999  75.99999999  86.99999999  97.99999999 108.99999999]\n",
      "1.870179339985957e-08\n",
      "\n",
      "progress\n",
      "[  1.  21.  32.  43.  54.  65.  76.  87.  98. 109.]\n",
      "4.58126749005559e-09\n",
      "\n",
      "progress\n",
      "[  1.  21.  32.  43.  54.  65.  76.  87.  98. 109.]\n",
      "1.2394988529583885e-09\n",
      "\n",
      "converged in 17 iterations to\n",
      "x value : [  1.  21.  32.  43.  54.  65.  76.  87.  98. 109.]\n",
      "out :  1.2394988529583885e-09\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = np.ones(10)\n",
    "y = np.array([1.,21.,32.,43.,54.,65.,76.,87.,98.,109.])\n",
    "def MSE_y(x):\n",
    "    return MSE(y,x)\n",
    "\n",
    "bfgs.find_minimum(MSE_y,start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
