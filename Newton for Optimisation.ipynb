{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method for Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Newton's method for optimisation (using hessian for the update). This converges very quickly for some functions, however this is effectively a stationary point finder, i.e. it is blind to actual minimisation of the objective function, so it could very well converge to a local maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a numpy array\n",
    "def f(x):\n",
    "    return np.array([(x[0]-10.)**2 +(x[1]-4.)**2])\n",
    " \n",
    "def MSE(y,x):\n",
    "    \n",
    "    return np.linalg.norm(y-x)# does not converge for this because looks for maximum\n",
    "    #return (y-x) ** 2 # this does converge as it is equivalent to coordinate-wise newton optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general class of linesearch iterative methods for continuous optimisation\n",
    "class LineSearch:\n",
    "    \n",
    "    def __init__(self,direction_func,backtracking,step):\n",
    "        self.direction = direction_func\n",
    "        self.backtracking = backtracking\n",
    "        self.step = step\n",
    "        \n",
    "        self.sigma = 0.1\n",
    "        self.tol = 0.00000001\n",
    "        self.rho = 1/2\n",
    "        \n",
    "    #takes function and starting x and returns new point\n",
    "    def take_step(self,f,x,step):\n",
    "        new_x = x + step*self.direction(f,x)\n",
    "        return new_x\n",
    "    \n",
    "    def Armijo_test(self,f,x,step,direction):\n",
    "        a = f(x+step*direction)\n",
    "        b = f(x) - self.sigma*step*np.dot(direction,direction)\n",
    "        return (a < b).all()\n",
    "    \n",
    "    #finds suitable step length using Armijo rule\n",
    "    def find_step(self,f,x,step):\n",
    "        iters =0\n",
    "        direction = self.direction(f,x)\n",
    "        while not self.Armijo_test(f,x,step,direction):\n",
    "            step = self.rho * step\n",
    "            iters +=1\n",
    "            if iters > 1000:\n",
    "                return None\n",
    "        return step\n",
    "        \n",
    "    #minimise f from starting point x\n",
    "    def find_minimum(self,f,x):\n",
    "        #executes steps until convergence\n",
    "        prev_x = 10*10*10\n",
    "        iters = 0\n",
    "        step = self.step\n",
    "        while np.linalg.norm(x-prev_x) > self.tol:\n",
    "            \n",
    "            #STEP LENGTH SEARCH\n",
    "            if self.backtracking :\n",
    "                step = self.find_step(f,x,step)\n",
    "            if step == None:\n",
    "                break\n",
    "                \n",
    "            # UPDATE\n",
    "            prev_x = x\n",
    "            x = self.take_step(f,x,step)\n",
    "            \n",
    "            #ITERATION CHECK\n",
    "            if iters > 10000:\n",
    "                print(\"iteration limit reached\")\n",
    "                return\n",
    "            else:\n",
    "                #if iters % 10 == 0:\n",
    "                print(\"progress\")\n",
    "                print(x)\n",
    "                print(f(x))\n",
    "                print()\n",
    "                iters +=1\n",
    "                \n",
    "        print(\"converged in %d iterations to\" %iters)\n",
    "        print(\"x value :\", x)\n",
    "        print(\"out : \", f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f,x):\n",
    "    grad = elementwise_grad(f)\n",
    "    try:\n",
    "        inv_hessian = np.linalg.inv(jacobian(grad)(x))\n",
    "    except:\n",
    "        inv_hessian = np.linalg.inv(jacobian(grad)(x)+ 0.01*np.identity(x.size))\n",
    "    \n",
    "    return np.negative(np.dot(inv_hessian,grad(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_optim = LineSearch(newton,backtracking = False, step=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress\n",
      "[10.  4.]\n",
      "[0.]\n",
      "\n",
      "progress\n",
      "[10.  4.]\n",
      "[0.]\n",
      "\n",
      "converged in 2 iterations to\n",
      "x value : [10.  4.]\n",
      "out :  [0.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([12.,112.])\n",
    "newton_optim.find_minimum(f,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress\n",
      "[1.64971832e+16 1.81469015e+17 2.72203523e+17 3.62938031e+17\n",
      " 4.53672539e+17 5.44407046e+17 6.35141554e+17 7.25876062e+17\n",
      " 8.16610570e+17 9.07345077e+17]\n",
      "1.7781025001808817e+18\n",
      "\n",
      "progress\n",
      "[-7.46043123e+31 -8.20647435e+32 -1.23097115e+33 -1.64129487e+33\n",
      " -2.05161859e+33 -2.46194231e+33 -2.87226602e+33 -3.28258974e+33\n",
      " -3.69291346e+33 -4.10323718e+33]\n",
      "8.041015996338684e+33\n",
      "\n",
      "progress\n",
      "[4.59624406e+47 5.05586847e+48 7.58380270e+48 1.01117369e+49\n",
      " 1.26396712e+49 1.51676054e+49 1.76955396e+49 2.02234739e+49\n",
      " 2.27514081e+49 2.52793423e+49]\n",
      "4.953932403718573e+49\n",
      "\n",
      "progress\n",
      "[2.47206331e+63 2.71926964e+64 4.07890446e+64 5.43853928e+64\n",
      " 6.79817410e+64 8.15780892e+64 9.51744374e+64 1.08770786e+65\n",
      " 1.22367134e+65 1.35963482e+65]\n",
      "2.664443917037564e+65\n",
      "\n",
      "progress\n",
      "[-1.29767838e+79 -1.42744622e+80 -2.14116933e+80 -2.85489244e+80\n",
      " -3.56861555e+80 -4.28233866e+80 -4.99606177e+80 -5.70978488e+80\n",
      " -6.42350799e+80 -7.13723110e+80]\n",
      "1.3986661510743614e+81\n",
      "\n",
      "progress\n",
      "[-9.39034433e+94 -1.03293788e+96 -1.54940681e+96 -2.06587575e+96\n",
      " -2.58234469e+96 -3.09881363e+96 -3.61528257e+96 -4.13175150e+96\n",
      " -4.64822044e+96 -5.16468938e+96]\n",
      "1.0121118556324268e+97\n",
      "\n",
      "progress\n",
      "[4.98156924e+110 5.47972617e+111 8.21958925e+111 1.09594523e+112\n",
      " 1.36993154e+112 1.64391785e+112 1.91790416e+112 2.19189047e+112\n",
      " 2.46587677e+112 2.73986308e+112]\n",
      "5.36924431326139e+112\n",
      "\n",
      "progress\n",
      "[2.37054274e+126 2.60759701e+127 3.91139552e+127 5.21519402e+127\n",
      " 6.51899253e+127 7.82279104e+127 9.12658954e+127 1.04303880e+128\n",
      " 1.17341866e+128 1.30379851e+128]\n",
      "2.55502282548439e+128\n",
      "\n",
      "progress\n",
      "[-7.57460033e+141 -8.33206037e+142 -1.24980905e+143 -1.66641207e+143\n",
      " -2.08301509e+143 -2.49961811e+143 -2.91622113e+143 -3.33282415e+143\n",
      " -3.74942716e+143 -4.16603018e+143]\n",
      "8.16406995352415e+143\n",
      "\n",
      "progress\n",
      "[3.30361467e+157 3.63397614e+158 5.45096421e+158 7.26795228e+158\n",
      " 9.08494035e+158 1.09019284e+159 1.27189165e+159 1.45359046e+159\n",
      " 1.63528926e+159 1.81698807e+159]\n",
      "inf\n",
      "\n",
      "progress\n",
      "[3.30361467e+157 3.63397614e+158 5.45096421e+158 7.26795228e+158\n",
      " 9.08494035e+158 1.09019284e+159 1.27189165e+159 1.45359046e+159\n",
      " 1.63528926e+159 1.81698807e+159]\n",
      "inf\n",
      "\n",
      "converged in 11 iterations to\n",
      "x value : [3.30361467e+157 3.63397614e+158 5.45096421e+158 7.26795228e+158\n",
      " 9.08494035e+158 1.09019284e+159 1.27189165e+159 1.45359046e+159\n",
      " 1.63528926e+159 1.81698807e+159]\n",
      "out :  inf\n"
     ]
    }
   ],
   "source": [
    "start = np.ones(10)\n",
    "y = np.array([1.,21.,32.,43.,54.,65.,76.,87.,98.,109.])\n",
    "def MSE_y(x):\n",
    "    return MSE(y,x)\n",
    "\n",
    "newton_optim.find_minimum(MSE_y,start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
